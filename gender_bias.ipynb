{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the data using Pickle (pickle.load(open(’data.p’, ’rb’)). Op- posed to the previous exercises, the data is not in a dataframe, but it is a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = pickle.load(open('/Users/jaspervogelzang/Documents/ADS Master/Data Mining/Lab Sessions/gender.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finally, some women journalists mentioned that information and communication technologies are responsible for creating new barriers for women in journalism because of the increased pace and pressure on their private lives.',\n",
       " 'still, it is necessary to first establish that language itself might play a bias-inducing role before assessing whether such bias can be overcome via another mechanism.',\n",
       " 'the process of constructing a national identity directly engages the construction of gender (charrad 2001; kandiyoti 1991; kim, puri, and kim-puri 2005; yuval-davis and anthias 1989), and sudan is no exception (hale 1996; nageeb 2004; tønnessen 2007).',\n",
       " 'what is the point of all this that these people do [pointing at his colleagues in the shop]—knives and women and who knows what else?',\n",
       " 'the first part of this article will question whether restorative justice mechanisms can be more conducive to the inclusion of women’s experiences.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender = [x.lower() for x in gender]\n",
    "gender[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tokenize the lowercased texts using NLTK (or Spacy – if you prefer), make sure only punctuations are removed, and train a word embeddings model (with an embedding size of 300 and a min count of 5) on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['finally', 'some', 'women', 'journalists', 'mentioned', 'that', 'information', 'and', 'communication', 'technologies', 'are', 'responsible', 'for', 'creating', 'new', 'barriers', 'for', 'women', 'in', 'journalism', 'because', 'of', 'the', 'increased', 'pace', 'and', 'pressure', 'on', 'their', 'private', 'lives'], ['still', 'it', 'is', 'necessary', 'to', 'first', 'establish', 'that', 'language', 'itself', 'might', 'play', 'a', 'bias', 'inducing', 'role', 'before', 'assessing', 'whether', 'such', 'bias', 'can', 'be', 'overcome', 'via', 'another', 'mechanism'], ['the', 'process', 'of', 'constructing', 'a', 'national', 'identity', 'directly', 'engages', 'the', 'construction', 'of', 'gender', 'charrad', '2001', 'kandiyoti', '1991', 'kim', 'puri', 'and', 'kim', 'puri', '2005', 'yuval', 'davis', 'and', 'anthias', '1989', 'and', 'sudan', 'is', 'no', 'exception', 'hale', '1996', 'nageeb', '2004', 'tønnessen', '2007'], ['what', 'is', 'the', 'point', 'of', 'all', 'this', 'that', 'these', 'people', 'do', 'pointing', 'at', 'his', 'colleagues', 'in', 'the', 'shop', 'knives', 'and', 'women', 'and', 'who', 'knows', 'what', 'else'], ['the', 'first', 'part', 'of', 'this', 'article', 'will', 'question', 'whether', 'restorative', 'justice', 'mechanisms', 'can', 'be', 'more', 'conducive', 'to', 'the', 'inclusion', 'of', 'women', 's', 'experiences']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "tokenized_texts = [tokenizer.tokenize(text) for text in gender]\n",
    "print(tokenized_texts[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8060130, 10645010)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIZE = 300 # dimensions of the embeddings\n",
    "SG = 1 # whether to use skip-gram or CBOW (we use skip-gram)\n",
    "WINDOW = 10 # the window size\n",
    "N_WORKERS = 1 # number of workers to use\n",
    "MIN_COUNT = 5\n",
    "\n",
    "model = Word2Vec(size=SIZE,\n",
    "                sg=SG,\n",
    "                window=WINDOW, \n",
    "                min_count=MIN_COUNT,\n",
    "                workers=N_WORKERS)\n",
    "\n",
    "model.build_vocab(tokenized_texts)\n",
    "\n",
    "model.train(tokenized_texts,\n",
    "           total_examples=model.corpus_count,\n",
    "           epochs=model.epochs) # grab some coffee while training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the most_similar() function for some random words to test the model. Does the model produce sensible results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('peers', 0.6688945293426514), ('supervisors', 0.6020901203155518), ('mentors', 0.6013844609260559), ('coworkers', 0.5986536145210266), ('ostracized', 0.5880432724952698), ('teachers', 0.5838507413864136), ('supervisor', 0.583335280418396), ('counselors', 0.5827285051345825), ('talented', 0.580131471157074), ('raters', 0.5754419565200806)]\n",
      "\n",
      " [('store', 0.8822088241577148), ('shops', 0.8123973608016968), ('elegant', 0.8078662753105164), ('bride', 0.805104672908783), ('owners', 0.8045867681503296), ('nights', 0.799264669418335), ('owner', 0.7969189882278442), ('flight', 0.7919496893882751), ('dresses', 0.7885294556617737), ('brides', 0.786113977432251)]\n",
      "\n",
      " [('identities', 0.6562486886978149), ('malleability', 0.6120238900184631), ('identifications', 0.6060490012168884), ('categorization', 0.5976635217666626), ('norwegianness', 0.5945264101028442), ('identification', 0.5914136171340942), ('fluidity', 0.5873438715934753), ('neurodivergent', 0.5601075291633606), ('politic', 0.5543736219406128), ('typified', 0.5542808175086975)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('colleagues'))\n",
    "print('\\n',model.wv.most_similar('shop'))\n",
    "print('\\n',model.wv.most_similar('identity'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate the similarity between man and king, and between woman and king. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.385419\n",
      "0.22175859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-78008352a689>:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  print(model.similarity('man', 'king'))\n",
      "<ipython-input-25-78008352a689>:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  print(model.similarity('woman', 'king'))\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('man', 'king'))\n",
    "print(model.similarity('woman', 'king'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38541901111602783\n",
      "0.22175857424736023\n"
     ]
    }
   ],
   "source": [
    "# or to use the spicy version:\n",
    "word_1 = model.wv['man']\n",
    "word_2 = model.wv['king']\n",
    "print(1 - cosine(word_1, word_2))\n",
    "\n",
    "word_1 = model.wv['woman']\n",
    "word_2 = model.wv['king']\n",
    "print(1 - cosine(word_1, word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Reproducing Wevers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For his analysis, Wevers relies on using lists of words within certain categories (money-related words/job-related words, etc). You can find English versions of the lists he uses in word cats.p. Load this data frame and inspect the list of occupations. Unlike English, compounds in Dutch are (normally) not written separately. Thus, in Dutch credit card is written as creditcard (or: kredietkaart). Why would that pose a possible problem if we want to reproduce Wevers’s analysis on English texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cats = pickle.load(open('/Users/jaspervogelzang/Documents/ADS Master/Data Mining/Lab Sessions/word_cats.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affect</th>\n",
       "      <th>posemo</th>\n",
       "      <th>negemo</th>\n",
       "      <th>social</th>\n",
       "      <th>family</th>\n",
       "      <th>cogproc</th>\n",
       "      <th>percept</th>\n",
       "      <th>body</th>\n",
       "      <th>work</th>\n",
       "      <th>leisure</th>\n",
       "      <th>money</th>\n",
       "      <th>relig</th>\n",
       "      <th>occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>protesting</td>\n",
       "      <td>incentive</td>\n",
       "      <td>destruction</td>\n",
       "      <td>chick</td>\n",
       "      <td>ma's</td>\n",
       "      <td>comply</td>\n",
       "      <td>squeez</td>\n",
       "      <td>pussy</td>\n",
       "      <td>dotcom</td>\n",
       "      <td>dnd</td>\n",
       "      <td>portfolio</td>\n",
       "      <td>goddess</td>\n",
       "      <td>accountant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pretty</td>\n",
       "      <td>luck</td>\n",
       "      <td>beaten</td>\n",
       "      <td>ma's</td>\n",
       "      <td>niece</td>\n",
       "      <td>luck</td>\n",
       "      <td>sand</td>\n",
       "      <td>wears</td>\n",
       "      <td>employee</td>\n",
       "      <td>vacation</td>\n",
       "      <td>sale</td>\n",
       "      <td>karma</td>\n",
       "      <td>actor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sighs</td>\n",
       "      <td>freeing</td>\n",
       "      <td>battl</td>\n",
       "      <td>lets</td>\n",
       "      <td>stepkid</td>\n",
       "      <td>unquestion</td>\n",
       "      <td>moist</td>\n",
       "      <td>hearts</td>\n",
       "      <td>paper</td>\n",
       "      <td>hobb</td>\n",
       "      <td>stores</td>\n",
       "      <td>pastor</td>\n",
       "      <td>actress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>warmth</td>\n",
       "      <td>pretty</td>\n",
       "      <td>protesting</td>\n",
       "      <td>son's</td>\n",
       "      <td>son's</td>\n",
       "      <td>pretty</td>\n",
       "      <td>warmth</td>\n",
       "      <td>asleep</td>\n",
       "      <td>earns</td>\n",
       "      <td>band</td>\n",
       "      <td>bets</td>\n",
       "      <td>temple</td>\n",
       "      <td>actuary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mooch</td>\n",
       "      <td>nicely</td>\n",
       "      <td>dumber</td>\n",
       "      <td>daddies</td>\n",
       "      <td>daddies</td>\n",
       "      <td>become</td>\n",
       "      <td>gloomy</td>\n",
       "      <td>gums</td>\n",
       "      <td>assign</td>\n",
       "      <td>skat</td>\n",
       "      <td>bank</td>\n",
       "      <td>holy</td>\n",
       "      <td>acupuncturist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>ty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>undesir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>trembl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>weaken</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>joking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1395 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          affect     posemo       negemo   social   family     cogproc  \\\n",
       "0     protesting  incentive  destruction    chick     ma's      comply   \n",
       "1         pretty       luck       beaten     ma's    niece        luck   \n",
       "2          sighs    freeing        battl     lets  stepkid  unquestion   \n",
       "3         warmth     pretty   protesting    son's    son's      pretty   \n",
       "4          mooch     nicely       dumber  daddies  daddies      become   \n",
       "...          ...        ...          ...      ...      ...         ...   \n",
       "1390          ty        NaN          NaN      NaN      NaN         NaN   \n",
       "1391     undesir        NaN          NaN      NaN      NaN         NaN   \n",
       "1392      trembl        NaN          NaN      NaN      NaN         NaN   \n",
       "1393      weaken        NaN          NaN      NaN      NaN         NaN   \n",
       "1394      joking        NaN          NaN      NaN      NaN         NaN   \n",
       "\n",
       "     percept    body      work   leisure      money    relig     occupation  \n",
       "0     squeez   pussy    dotcom       dnd  portfolio  goddess     accountant  \n",
       "1       sand   wears  employee  vacation       sale    karma          actor  \n",
       "2      moist  hearts     paper      hobb     stores   pastor        actress  \n",
       "3     warmth  asleep     earns      band       bets   temple        actuary  \n",
       "4     gloomy    gums    assign      skat       bank     holy  acupuncturist  \n",
       "...      ...     ...       ...       ...        ...      ...            ...  \n",
       "1390     NaN     NaN       NaN       NaN        NaN      NaN            NaN  \n",
       "1391     NaN     NaN       NaN       NaN        NaN      NaN            NaN  \n",
       "1392     NaN     NaN       NaN       NaN        NaN      NaN            NaN  \n",
       "1393     NaN     NaN       NaN       NaN        NaN      NaN            NaN  \n",
       "1394     NaN     NaN       NaN       NaN        NaN      NaN            NaN  \n",
       "\n",
       "[1395 rows x 13 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Make a list of male-related words (he, his, male, etc.) and of female- related words. Get the mean embeddings of those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean embedding for male words:  (300,)\n",
      "Mean embedding for female words:  (300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['female', 'her', 'feminine', 'women', 'lady', 'sister', 'granny', 'daughter']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male = ['male', 'men','boy', 'guy', 'his', 'him', 'dude']\n",
    "female = ['female', 'her', 'feminine', 'women', 'lady', 'sister', 'granny', 'daughter']\n",
    "\n",
    "words = [word for word in male if word in model.wv.vocab] # checks if word is in vocabulary (i.e. has been seen by the model before)\n",
    "mean_embedding_male = np.mean([model.wv[words] for words in male], axis=0)\n",
    "print('Mean embedding for male words: ', mean_embedding_male.shape)\n",
    "\n",
    "words = [word for word in female if word in model.wv.vocab] # checks if word is in vocabulary (i.e. has been seen by the model before)\n",
    "mean_embedding_female = np.mean([model.wv[words] for words in female], axis=0)\n",
    "print('Mean embedding for female words: ', mean_embedding_female.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To get an indication of the gender bias related to a certain category, Wevers uses the following method:\n",
    "First, he loops over all the words within a certain category (e.g. occupa- tions). For each word in the category, he calculates (1) the (Frobenius- based) distance between that word and the mean embedding of the male related words, and (2) the distance between that word and the mean embedding of the female related words.\n",
    "Then, (3), he calculates, for each word in the category, the difference between (1) and (2) by subtracting them, This could be seen as the gender bias of a specific word.\n",
    "Finally, to get the gender bias within a certain category, he takes the mean of the gender bias for each word in the category.\n",
    "Try to implement this approach using the words in the column occupation. When you loop over the words within a category, also store the word and the bias of that words in a dictionary, and use that to make a data frame (also see the section Most distinctive words on how to make a data frame).\n",
    "What occupations are most biased towards men, and what occupations are most biased towards women? What do the results say about the gender bias in these journals? And what do they say about the word embeddings method in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
